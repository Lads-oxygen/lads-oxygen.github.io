@def title = "LLM Evaluation"
@def maxtoclevel=2
@def mintoclevel=2

{{insert navbar.html}}

@@toc-container
@@toc
\tableofcontents
@@
@@


@@div

# A Comparison of Large Language Model (LLM) Evaluation Services

@@date
September 2, 2024
@@

Before we begin discussing different LLM evaluation services, let's first review what LLM evaluation is and why it is important.

## Large Language Model (LLM) Evaluation

LLM evaluation refers to the process of gauging how well a model's outputs are aligned with human expectations. This involves assessing the model's performance across various tasks as well as ensuring that its interactions remain ethical and unbiased.

As the deployment of LLMs has become more widespread, so has the need for rigorous and comprehensive evaluation methods. The increase in complexity and range of tasks that modern LLMs are used for has called for new evaluation methods, and integration of LLMs across various industries has also driven the need to assess their ethical implications. Of particular interest is the implementation of robust and systematic evaluation frameworks.

In practice, we make use of:

- unit test cases - individual tests that examine how well a language model handles specific tasks e.g. assessing whether an LLM can generate a corresct answer to a specific factual question.
- evaluation metrics - quantitative measures of an LLM's performance across a range of tasks e.g. verbosity.

@@figure
\fig{./eval_metric.svg}
@@

Next we consider the different types of scorers that we can use.

## Evaluation Methods

### Human Evaluation

This is the most basic setup. Individuals manually judge LLM responses based on a set of criteria specific to each evaluation metric. In practice, this method often relies on the LLM application's end-users, instead of recruiting dedicated evaluators. The users provide feedback through mechanisms such as thumbs up/down buttons and binary choice buttons.

### Computation-Based Metrics

Computation-based metrics are quantitative measures for evaluating LLMs by comparing their outputs to predefined ground truths.

- **Lexicon-based metrics:** Use maths to calculate the string similarities between LLM-generated results and ground truth, such as Exact Match and ROUGE.
- **Count-based metrics:** Aggregate the number of rows that hit or miss certain ground-truth labels, such as F1-score, Accuracy, and Tool Name Match.
- **Embedding-based metrics:** Calculate the distance/angle between the LLM-generated results and ground truth in the embedding space, reflecting their level of similarity.

Computation-based metrics only capture surface level differences, as they cannot capture subtleties such as tone and style. Moreover, they may penalise small wording differences that convey the same meaning. To see this, let's consider the precision metric defined as:

$$
\text{Precision} = \frac{\text{Number of common words between output and reference
}}{\text{Total number of words in output}}
$$

against the following two sentences (where reference is the ideal/expected output):

**Reference Sentence:** "The cat sat on the mat."

**Generated Output:** "The feline rested on the carpet."

Despite the two sentences conveying the same meaning, the precision score would be low due to the lack of common words.

### LLM-Based Evaluation

This a relatively new evaluation technique, introduced in the LLM-as-a-Judge paper [^1]. In this method, we use an LLM to evaluate the outputs of another LLM based on the same set of criteria as in human evaluation. The LLM judge used is typically a large generalist model or a smaller model fine-tuned for evaluation. The LLM judge can provide a score for each output, as well as a justification for the score.



**Limitations:**

- Position bias - when comparing two models pair-wise, the first model is typically favoured.
- Self-enhancement bias - LLM judges may favour the answers generated by themselves.
- Verbosity bias - LLM judge favours longer, verbose responses, even if they are not as clear, high-quality, or accurate as shorter alternatives. As an oversimplification, we can see that this is because longer responses are more likely to contain the desired answer, as they contain more information.

To counter these limitations, we can:

- Use point-wise evaluation [^2].
- Use various LLMs as judges or use an LLM that has been fine-tuned for evaluation.
- Include a verbosity/concision metric to penalise overly verbose answers.

Note that the length of the output, typically determines the main cost of the LLM call and it is also linearly correlated to the LLM's latency [^3]. Hence, verbosity is actually one the most important metrics to evaluate.


@@figure
\fig{./point-wise_vs_pair-wise.svg}
@@

## Human vs Computational vs LLM-Based Evaluation

Human evaluation is the most accurate and nuanced evaluation method. However, it is also the most expensive and time-consuming. Computational metrics are generally the fastest and cheapest, and LLM-based metrics strike a good middle ground between the two.

As previously discussed, computational metrics are not as accurate or nuanced and they require a reference answer. By contrast, most LLM-based metrics are **reference-free**, besides a handful such as correctness or contextual recall/precision. In practice, this makes LLM-based metrics more suitable for live applications, as they can be used on dynamic datasets (collection of test cases) without the need to provide reference answers, which can be time-consuming and costly to obtain.

Moreover, LLM-human agreement can achieve or even surpass the human-human agreement of around 80% [^1], as for subjective tasks, LLMs can actually be more consistent than humans. The ability of LLM-based metrics to provide reasoning for their judgement is also a significant advantage. It seems like LLM-based metrics yield all the benefits of human evaluation, without the cost and time constraints.

## Comparison of Evaluation Services

We will consider the evaluation services provided by Amazon and Google's AI platforms, Amazon Bedrock and Vertex AI respectively, as well as an open source alternative, Deepeval, provided by Confident AI. The things we look for in an evaluation service are:

- Support for LLM-based metrics.
- The ability to define custom LLM-based metrics.

@@my-table
| Evaluation Service              | Deepeval                                                                                               | Vertex AI                                                                                                                                                                             | Bedrock                                                                  |
| ------------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| Evaluation methods              | ~~~<ul><li>LLM-based metrics </li><li>Computational metrics </li><li>Human feedback </li></ul>~~~      | ~~~<ul><li>LLM-based metrics </li><li>Computational metrics </li></ul>~~~                                                                                                             | ~~~<ul><li>Computational metrics </li><li>Human evaluation </li></ul>~~~ |
| The LLM used as a judge         | ~~~<ul><li>Use any model accessible through an API </li><li>Not fine-tuned for evaluation </li><ul>~~~ | Custom model used for evaluation, most likely based on gemini (for example) and fine-tuned for evaluation                                                                             | N/A                                                                      |
| Custom LLM-based metric support | Yes:~~~<ul><li>Includes RAG metrics </li><li>Includes conversational metrics</li></ul>~~~              | Yes:~~~<ul><li>Includes RAG metrics</li><li>Includes conversational metrics (new)</li></ul>~~~                                                                                        | No                                                                       |
| Custom LLM support              | Yes:~~~<ul><li>online evaluations</li><li>Offline evaluations</li></ul>~~~                             | Yes:~~~<ul><li>Offline evaluations</li></ul>~~~                                                                                                                                       | No: ~~~<ul><li>Can only evaluate LLMs available on Bedrock</li></ul>~~~  |
| Source availability             | Open source, except for visualisation dashboard (Confident AI)                                         | Closed source                                                                                                                                                                         | Closed source                                                            |
| Other caveats/advantages        | ~~~<ul><li>Most metrics are backed by research~~~ [^4] ~~~</li></ul>~~~                                | ~~~<ul><li>Non-english inputs have worse evaluation quality </li><li>Pointwise or pairwise evaluation </li><li>Most metrics require context parameter (unlike Deepeval) </li></ul>~~~ |                                                                          |
@@

Based on our previous discussion, we can see that Amazon Bedrock's evaluation service is the most primitive, as it lacks both LLM-based metrics as well the ability to evaluate LLMs that are not available on the platform. Vertex AI is more advanced, offering both of these features as well as supporting custom LLM-based metrics. Unfortunately, Google's implementation has higher friction when compared with Deepeval. More specifically, to define custom metrics in Vertex AI one has to provide the raw prompt, whereas in Deepeval one only needs to pass the metric's criteria to G-Eval [^4].

In addition, Vertex AI only supports online evaluations for LLMs available on the Vertex AI platform, whereas Deepeval supports online evaluations for any LLM accessible through an API. Online evaluation refers to the process of evaluating models in real-time as they generate outputs, providing immediate feedback. This is particularly useful in applications requiring instant model adjustments, such as chatbots. In contrast, offline evaluation uses a historical dataset to assess model performance retrospectively, offering insights for batch processing or benchmarking.

Another important difference between Vertex AI and Deepeval, is that Vertex AI uses a fine-tuned model for evaluation (likely based on Gemini), while Deepeval uses any (generalist) model accessible through an API. In testing, both of these services are comparably accurate and consistent, however with Deepeval one has to be wary of using the same LLM for both generation and evaluation, as this can lead to the aforementioned self-enhancement bias.

DeepEval, also has the advantage of being open source and having an active community that contributes to the project, ensuring it has all the latest features.

## Conclusion

In this article we've reviewed LLM evaluation and compared the key differences between a couple evaluation services. We've seen that LLM-based metrics are more suitable for live applications, and that DeepEval, has the most features and is the easiest to use. However, it is important to note that these products are quite new and constantly evolving, with Vertex AI's evaluation service still being in preview. By the time you read this article, the landscape may have changed.

## Code Generation (Further Reading)

Code generation is one of the most useful applications of LLMs. So naturally, evaluating the quality of generated code is of great interest. For the keen among you, I would recommend checking out this [paper](https://arxiv.org/abs/2107.03374), to see how we might evaluate functional correctness of generated code. Essentially we execute the generated code on a set of generated test cases in a sandbox environment, and check if the output matches the expected output. I would also recommend reading this [paper](https://github.com/microsoft/CodeT/tree/main/CodeT), which introduces a method for improving generated code. The idea is to generate multiple candidates and select the best one. In fact, this approach can also be used to improve the quality of generated text.

## References

@@references
[^1]: [LLM-as-a-Judge](https://example.com/llm-as-a-judge)

[^2]: [Latency](https://www.taivo.ai/__llm-latency-is-linear-in-output-token-count/)

[^3]: [Point-wise vs Pair-wise](https://arxiv.org/pdf/2406.12319)

[^4]: [G-Eval](https://arxiv.org/abs/2303.16634)
@@

@@uncited-references
[Confident AI Blog](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method)

[Vertex AI Evaluation](https://cloud.google.com/vertex-ai/docs/evaluation/introduction)

[Bedrock Metrics](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-tasks.html)
@@

@@